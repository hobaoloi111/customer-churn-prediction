"""
B∆Ø·ªöC 1: L∆ØU V√Ä T·∫¢I M√î H√åNH ML
H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng joblib ƒë·ªÉ persist (l∆∞u) m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
"""

import joblib
import pickle
import json
from pathlib import Path
from datetime import datetime
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

print("\n" + "="*80)
print("B∆Ø·ªöC 1: L∆ØU V√Ä T·∫¢I M√î H√åNH")
print("="*80)

# ============================================================================
# PH·∫¶N 1: L∆ØU M√î H√åNH (Model Serialization)
# ============================================================================

print("\n[PH·∫¶N 1] L∆ØU M√î H√åNH ƒê∆Ø·ªöC HU·∫§N LUY·ªÜN")
print("-" * 80)

# X√°c ƒë·ªãnh th∆∞ m·ª•c l∆∞u model
project_root = Path(__file__).parent.parent
models_dir = project_root / 'models'
models_dir.mkdir(exist_ok=True)

print(f"\n[1.1] Th∆∞ m·ª•c l∆∞u model: {models_dir}")

# ============================================================================
# PH·∫¶N 2: T·∫†O & HU·∫§N LUY·ªÜN M√î H√åNH (Gi·ªëng trong model_training.py)
# ============================================================================

print("\n[PH·∫¶N 2] LOAD D·ªÆ LI·ªÜU & HU·∫§N LUY·ªÜN M√î H√åNH")
print("-" * 80)

# Load d·ªØ li·ªáu
data_file = project_root / 'data' / 'customer_churn_data.csv'
df = pd.read_csv(data_file)

# Chia X, y
target_col = 'churn'
X = df.drop(columns=['customer_id', target_col])
y = df[target_col]

# Chia train/test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Preprocessing
numerical_cols = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()

X_train_processed = X_train.copy()
X_test_processed = X_test.copy()

# Label Encoding
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X_train_processed[col] = le.fit_transform(X_train[col])
    X_test_processed[col] = le.transform(X_test[col])
    label_encoders[col] = le

# StandardScaling
scaler = StandardScaler()
X_train_processed[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test_processed[numerical_cols] = scaler.transform(X_test[numerical_cols])

print(f"\n[2.1] Dataset loaded: {X_train_processed.shape}")
print(f"[2.2] Preprocessing complete")

# Hu·∫•n luy·ªán 3 m√¥ h√¨nh
print(f"\n[2.3] Training models...")

lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_processed, y_train)
print(f"  ‚úì Logistic Regression trained")

dt_model = DecisionTreeClassifier(max_depth=7, min_samples_split=10, random_state=42)
dt_model.fit(X_train_processed, y_train)
print(f"  ‚úì Decision Tree trained")

rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, random_state=42, n_jobs=-1)
rf_model.fit(X_train_processed, y_train)
print(f"  ‚úì Random Forest trained")

# ============================================================================
# PH·∫¶N 3: L∆ØU C√ÅC MODELS B·∫∞NG JOBLIB
# ============================================================================

print("\n[PH·∫¶N 3] L∆ØU M√î H√åNH B·∫∞NG JOBLIB")
print("-" * 80)

# 3.1 L∆∞u t·ª´ng model
models_to_save = {
    'logistic_regression': lr_model,
    'decision_tree': dt_model,
    'random_forest': rf_model
}

saved_models = {}
for model_name, model in models_to_save.items():
    model_path = models_dir / f'{model_name}.pkl'
    joblib.dump(model, model_path)
    saved_models[model_name] = str(model_path)
    print(f"\n[3.1] Saved: {model_name}")
    print(f"  Location: {model_path}")
    print(f"  Size: {model_path.stat().st_size / 1024:.2f} KB")

# 3.2 L∆∞u preprocessor (Scaler & Encoders)
print(f"\n[3.2] Saving preprocessors...")

# L∆∞u Scaler
scaler_path = models_dir / 'scaler.pkl'
joblib.dump(scaler, scaler_path)
print(f"  ‚úì Scaler: {scaler_path}")

# L∆∞u Label Encoders
encoders_path = models_dir / 'label_encoders.pkl'
joblib.dump(label_encoders, encoders_path)
print(f"  ‚úì Label Encoders: {encoders_path}")

# 3.3 L∆∞u Feature Names (quan tr·ªçng!)
print(f"\n[3.3] Saving feature information...")

feature_info = {
    'feature_names': X_train.columns.tolist(),
    'numerical_cols': numerical_cols,
    'categorical_cols': categorical_cols
}

feature_info_path = models_dir / 'feature_info.json'
with open(feature_info_path, 'w') as f:
    json.dump(feature_info, f, indent=2)
print(f"  ‚úì Feature Info: {feature_info_path}")

# ============================================================================
# PH·∫¶N 4: L∆ØU METADATA (TH√îNG TIN M√î H√åNH)
# ============================================================================

print(f"\n[PH·∫¶N 4] L∆ØU METADATA")
print("-" * 80)

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# T√≠nh metrics tr√™n test set
metrics_dict = {}
for model_name, model in models_to_save.items():
    y_pred = model.predict(X_test_processed)
    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]
    
    metrics_dict[model_name] = {
        'accuracy': float(accuracy_score(y_test, y_pred)),
        'f1_score': float(f1_score(y_test, y_pred)),
        'roc_auc': float(roc_auc_score(y_test, y_pred_proba))
    }

# T·∫°o metadata object
metadata = {
    'timestamp': datetime.now().isoformat(),
    'models': {
        'logistic_regression': {
            'path': saved_models['logistic_regression'],
            'type': 'LogisticRegression',
            'hyperparameters': {
                'random_state': 42,
                'max_iter': 1000
            },
            'metrics': metrics_dict['logistic_regression']
        },
        'decision_tree': {
            'path': saved_models['decision_tree'],
            'type': 'DecisionTreeClassifier',
            'hyperparameters': {
                'max_depth': 7,
                'min_samples_split': 10,
                'random_state': 42
            },
            'metrics': metrics_dict['decision_tree']
        },
        'random_forest': {
            'path': saved_models['random_forest'],
            'type': 'RandomForestClassifier',
            'hyperparameters': {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 10,
                'random_state': 42
            },
            'metrics': metrics_dict['random_forest']
        }
    },
    'preprocessors': {
        'scaler': str(scaler_path),
        'label_encoders': str(encoders_path)
    },
    'feature_info': str(feature_info_path),
    'training_data': {
        'total_samples': len(df),
        'train_samples': len(X_train),
        'test_samples': len(X_test),
        'churn_rate': float(y.sum() / len(y))
    }
}

# L∆∞u metadata v√†o JSON
metadata_path = models_dir / 'model_metadata.json'
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=2)

print(f"\n[4.1] Metadata saved: {metadata_path}")
print(f"\n[4.2] Model Performance Summary:")
for model_name, metrics in metrics_dict.items():
    print(f"\n  {model_name}:")
    print(f"    - Accuracy: {metrics['accuracy']:.4f}")
    print(f"    - F1 Score: {metrics['f1_score']:.4f}")
    print(f"    - ROC-AUC:  {metrics['roc_auc']:.4f}")

# ============================================================================
# PH·∫¶N 5: T·∫¢I MODEL (DEMO)
# ============================================================================

print("\n" + "="*80)
print("PH·∫¶N 5: DEMO - T·∫¢I M√î H√åNH V√Ä PREDICT")
print("="*80)

# 5.1 T·∫£i metadata
print(f"\n[5.1] Loading metadata...")
with open(metadata_path, 'r') as f:
    loaded_metadata = json.load(f)
print(f"  ‚úì Metadata loaded")
print(f"  Timestamp: {loaded_metadata['timestamp']}")

# 5.2 T·∫£i models
print(f"\n[5.2] Loading models...")

loaded_models = {}
for model_name in ['logistic_regression', 'decision_tree', 'random_forest']:
    model_path = models_dir / f'{model_name}.pkl'
    loaded_models[model_name] = joblib.load(model_path)
    print(f"  ‚úì {model_name} loaded")

# 5.3 T·∫£i preprocessors
print(f"\n[5.3] Loading preprocessors...")

loaded_scaler = joblib.load(scaler_path)
loaded_encoders = joblib.load(encoders_path)

with open(feature_info_path, 'r') as f:
    loaded_feature_info = json.load(f)

print(f"  ‚úì Scaler loaded")
print(f"  ‚úì Label Encoders loaded")
print(f"  ‚úì Feature Info loaded")

# 5.4 Demo prediction
print(f"\n[5.4] DEMO PREDICTION")
print(f"  S·ª≠ d·ª•ng m·∫´u t·ª´ test set...")

# L·∫•y 1 m·∫´u t·ª´ test set
demo_sample = X_test.iloc[0:1].copy()
print(f"\n  Sample input:")
print(demo_sample)

# Preprocessing
demo_processed = demo_sample.copy()
for col in loaded_feature_info['categorical_cols']:
    demo_processed[col] = loaded_encoders[col].transform(demo_sample[col])

demo_processed[loaded_feature_info['numerical_cols']] = loaded_scaler.transform(
    demo_sample[loaded_feature_info['numerical_cols']]
)

print(f"\n  Predictions:")
for model_name, model in loaded_models.items():
    pred = model.predict(demo_processed)[0]
    proba = model.predict_proba(demo_processed)[0]
    
    print(f"\n  {model_name}:")
    print(f"    - Prediction: {pred} ({'Stayed' if pred == 0 else 'Churned'})")
    print(f"    - Probability: Stayed={proba[0]:.2%}, Churned={proba[1]:.2%}")

print(f"\n  Ground Truth: {y_test.iloc[0]} ({'Stayed' if y_test.iloc[0] == 0 else 'Churned'})")

# ============================================================================
# PH·∫¶N 6: BEST PRACTICES
# ============================================================================

print("\n" + "="*80)
print("PH·∫¶N 6: BEST PRACTICES KHI L∆ØU/T·∫¢I M√î H√åNH")
print("="*80)

best_practices = """
‚úÖ DO's (N√™n l√†m):
  1. Lu√¥n l∆∞u preprocessing objects (scaler, encoders) c√πng v·ªõi model
  2. L∆∞u feature names ƒë·ªÉ ƒë·∫£m b·∫£o input ƒë√∫ng th·ª© t·ª±
  3. L∆∞u metadata (accuracy, timestamp, hyperparameters)
  4. D√πng joblib thay v√¨ pickle (nhanh h∆°n cho numpy arrays)
  5. Ki·ªÉm tra version th∆∞ vi·ªán (scikit-learn, pandas, numpy)
  6. L∆∞u ri√™ng cho m·ªói m√¥ h√¨nh ƒë·ªÉ d·ªÖ qu·∫£n l√Ω
  7. T√™n file r√µ r√†ng v·ªõi timestamp (e.g., model_20231226_v2.pkl)

‚ùå DON'Ts (Kh√¥ng n√™n l√†m):
  1. Kh√¥ng l∆∞u m√¥ h√¨nh m√† kh√¥ng c√≥ scaler
  2. Kh√¥ng x√≥a label encoders - c·∫ßn d√πng l·∫°i khi predict
  3. Kh√¥ng l∆∞u model t·ª´ pickle (deprecated, b·∫£o m·∫≠t k√©m)
  4. Kh√¥ng qu√™n feature order
  5. Kh√¥ng test l·∫°i model sau load tr∆∞·ªõc d√πng production

‚ö†Ô∏è L∆∞u √Ω:
  - Model size l·ªõn? D√πng cloud storage (S3, GCS)
  - Nhi·ªÅu version? D√πng version control (DVC, MLflow)
  - Production? C·∫ßn monitoring & versioning (MLOps)
  - B·∫£o m·∫≠t? Encrypt model files tr∆∞·ªõc l∆∞u
"""

print(best_practices)

# ============================================================================
# PH·∫¶N 7: HELPER FUNCTIONS
# ============================================================================

print("\n[PH·∫¶N 7] HELPER FUNCTIONS - T√°i s·ª≠ d·ª•ng")
print("-" * 80)

# T·∫°o file helper.py
helper_code = '''
"""
Helper functions ƒë·ªÉ l∆∞u/t·∫£i m√¥ h√¨nh m·ªôt c√°ch d·ªÖ d√†ng
"""

import joblib
import json
from pathlib import Path
from datetime import datetime

def save_model_package(model, scaler, encoders, feature_info, 
                       model_name='my_model', models_dir='models'):
    """
    L∆∞u m√¥ h√¨nh + preprocessors + metadata m·ªôt l√∫c
    
    Args:
        model: Trained model (sklearn)
        scaler: StandardScaler object
        encoders: Dict of LabelEncoders
        feature_info: Dict with feature_names, numerical_cols, categorical_cols
        model_name: T√™n model (str)
        models_dir: Th∆∞ m·ª•c l∆∞u (str or Path)
    """
    models_dir = Path(models_dir)
    models_dir.mkdir(exist_ok=True)
    
    # L∆∞u model
    model_path = models_dir / f'{model_name}_model.pkl'
    joblib.dump(model, model_path)
    
    # L∆∞u scaler
    scaler_path = models_dir / f'{model_name}_scaler.pkl'
    joblib.dump(scaler, scaler_path)
    
    # L∆∞u encoders
    encoders_path = models_dir / f'{model_name}_encoders.pkl'
    joblib.dump(encoders, encoders_path)
    
    # L∆∞u feature info
    feature_info_path = models_dir / f'{model_name}_features.json'
    with open(feature_info_path, 'w') as f:
        json.dump(feature_info, f, indent=2)
    
    # T·∫°o metadata
    metadata = {
        'timestamp': datetime.now().isoformat(),
        'model_name': model_name,
        'files': {
            'model': str(model_path),
            'scaler': str(scaler_path),
            'encoders': str(encoders_path),
            'features': str(feature_info_path)
        }
    }
    
    metadata_path = models_dir / f'{model_name}_metadata.json'
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"‚úì Model package saved: {model_name}")
    return metadata

def load_model_package(model_name='my_model', models_dir='models'):
    """
    T·∫£i m√¥ h√¨nh + preprocessors ƒë·∫ßy ƒë·ªß
    
    Args:
        model_name: T√™n model (str)
        models_dir: Th∆∞ m·ª•c l∆∞u (str or Path)
    
    Returns:
        Tuple: (model, scaler, encoders, feature_info)
    """
    models_dir = Path(models_dir)
    
    # T·∫£i model
    model_path = models_dir / f'{model_name}_model.pkl'
    model = joblib.load(model_path)
    
    # T·∫£i scaler
    scaler_path = models_dir / f'{model_name}_scaler.pkl'
    scaler = joblib.load(scaler_path)
    
    # T·∫£i encoders
    encoders_path = models_dir / f'{model_name}_encoders.pkl'
    encoders = joblib.load(encoders_path)
    
    # T·∫£i feature info
    feature_info_path = models_dir / f'{model_name}_features.json'
    with open(feature_info_path, 'r') as f:
        feature_info = json.load(f)
    
    print(f"‚úì Model package loaded: {model_name}")
    return model, scaler, encoders, feature_info
'''

helper_path = project_root / 'src' / 'model_helper.py'
with open(helper_path, 'w', encoding='utf-8') as f:
    f.write(helper_code)

print(f"\n[7.1] Helper functions saved: {helper_path}")
print(f"\nUsage example:")
print(f"""
from src.model_helper import save_model_package, load_model_package

# L∆∞u model
save_model_package(rf_model, scaler, encoders, feature_info, 
                   model_name='churn_rf_v1', models_dir='models')

# T·∫£i model
model, scaler, encoders, features = load_model_package(
    model_name='churn_rf_v1', models_dir='models')
""")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*80)
print("‚úÖ SUMMARY - B∆Ø·ªöC 1 HO√ÄN T·∫§T")
print("="*80)

summary = f"""
üìÅ FILES SAVED:
  ‚úì {models_dir / 'logistic_regression.pkl'} ({(models_dir / 'logistic_regression.pkl').stat().st_size / 1024:.1f} KB)
  ‚úì {models_dir / 'decision_tree.pkl'} ({(models_dir / 'decision_tree.pkl').stat().st_size / 1024:.1f} KB)
  ‚úì {models_dir / 'random_forest.pkl'} ({(models_dir / 'random_forest.pkl').stat().st_size / 1024:.1f} KB)
  ‚úì {scaler_path}
  ‚úì {encoders_path}
  ‚úì {feature_info_path}
  ‚úì {metadata_path}
  ‚úì {helper_path}

üéØ NEXT STEP:
  B∆∞·ªõc 2 - Build CLI Application (d√πng c√°c model ƒë√£ l∆∞u)
"""

print(summary)
