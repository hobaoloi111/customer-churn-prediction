"""
Complete ML Pipeline for Customer Churn Prediction
B√†i to√°n: Binary Classification - D·ª± ƒëo√°n kh√°ch h√†ng r·ªùi ƒëi

C√°c b∆∞·ªõc:
1. Load d·ªØ li·ªáu t·ª´ CSV
2. Chia train/test split (80/20)
3. Preprocessing: Scaling + Encoding
4. Hu·∫•n luy·ªán 3 m√¥ h√¨nh ML
5. ƒê√°nh gi√° tr√™n test set
6. Visualization k·∫øt qu·∫£
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Scikit-learn imports
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report, 
                             roc_auc_score, roc_curve)

# C·∫•u h√¨nh matplotlib
plt.switch_backend('Agg')
sns.set_style("whitegrid")

# ============================================================================
# B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU T·ª™ CSV")
print("="*80)

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file
project_root = Path(__file__).parent.parent
data_file = project_root / 'data' / 'customer_churn_data.csv'

print(f"\n[1.1] ƒêang load d·ªØ li·ªáu t·ª´: {data_file}")
df = pd.read_csv(data_file)

print(f"[1.2] Dataset shape: {df.shape}")
print(f"[1.3] C·ªôt d·ªØ li·ªáu:")
print(df.columns.tolist())
print(f"\n[1.4] 5 h√†ng ƒë·∫ßu ti√™n:")
print(df.head())

# ============================================================================
# B∆Ø·ªöC 2: PH√ÇN T√ÅCH FEATURES V√Ä TARGET
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 2: PH√ÇN T√ÅCH FEATURES V√Ä TARGET")
print("="*80)

# X√°c ƒë·ªãnh target column
target_col = 'churn'
X = df.drop(columns=['customer_id', target_col])  # B·ªè ID v√† target
y = df[target_col]

print(f"\n[2.1] Target distribution:")
print(y.value_counts())
print(f"  - Churn rate: {(y.sum()/len(y))*100:.1f}%")

print(f"\n[2.2] Features s·∫Ω d√πng: {X.columns.tolist()}")
print(f"[2.3] S·ªë features: {X.shape[1]}")

# ============================================================================
# B∆Ø·ªöC 3: CHIA TRAIN/TEST SPLIT (80/20)
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 3: CHIA TRAIN/TEST SPLIT (80/20)")
print("="*80)

# Chia d·ªØ li·ªáu - stratify ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá class gi·ªëng nhau
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,           # 20% test
    random_state=42,         # Reproducible
    stratify=y               # Gi·ªØ t·ª∑ l·ªá churn gi·ªëng
)

print(f"\n[3.1] Train set size: {X_train.shape[0]}")
print(f"[3.2] Test set size: {X_test.shape[0]}")
print(f"\n[3.3] Train set target distribution:")
print(y_train.value_counts())
print(f"\n[3.4] Test set target distribution:")
print(y_test.value_counts())

# ============================================================================
# B∆Ø·ªöC 4: PREPROCESSING D·ªÆ LI·ªÜU
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 4: PREPROCESSING D·ªÆ LI·ªÜU (SCALING + ENCODING)")
print("="*80)

# 4.1 X√°c ƒë·ªãnh c·ªôt numerical v√† categorical
numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()

print(f"\n[4.1] Numerical columns: {numerical_cols}")
print(f"[4.2] Categorical columns: {categorical_cols}")

# 4.2 X·ª≠ l√Ω categorical features - Label Encoding
print(f"\n[4.3] Label Encoding c√°c categorical features:")
X_train_processed = X_train.copy()
X_test_processed = X_test.copy()

label_encoders = {}  # L∆∞u encoder ƒë·ªÉ apply tr√™n test
for col in categorical_cols:
    le = LabelEncoder()
    # Fit tr√™n train set
    X_train_processed[col] = le.fit_transform(X_train[col])
    # Transform test set v·ªõi encoder t·ª´ train
    X_test_processed[col] = le.transform(X_test[col])
    label_encoders[col] = le
    
    print(f"  {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}")

# 4.3 X·ª≠ l√Ω numerical features - StandardScaler (chu·∫©n h√≥a)
print(f"\n[4.4] Standardization (chu·∫©n h√≥a) numerical features:")
scaler = StandardScaler()
X_train_processed[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test_processed[numerical_cols] = scaler.transform(X_test[numerical_cols])

print(f"  Mean sau chu·∫©n h√≥a: {X_train_processed[numerical_cols].mean().round(3).tolist()}")
print(f"  Std sau chu·∫©n h√≥a: {X_train_processed[numerical_cols].std().round(3).tolist()}")

print(f"\n[4.5] D·ªØ li·ªáu sau preprocessing:")
print(f"  X_train shape: {X_train_processed.shape}")
print(f"  X_test shape: {X_test_processed.shape}")
print(f"  5 h√†ng ƒë·∫ßu ti√™n (X_train_processed):")
print(X_train_processed.head())

# ============================================================================
# B∆Ø·ªöC 5: HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 5: HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH")
print("="*80)

# Dictionary ƒë·ªÉ l∆∞u k·∫øt qu·∫£
models = {}

# 5.1 Logistic Regression
print(f"\n[5.1] Training Logistic Regression...")
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_processed, y_train)
models['Logistic Regression'] = lr_model
print(f"  ‚úì Training complete")

# 5.2 Decision Tree
print(f"\n[5.2] Training Decision Tree Classifier...")
dt_model = DecisionTreeClassifier(
    max_depth=7,           # H·∫°n ch·∫ø ƒë·ªô s√¢u ƒë·ªÉ tr√°nh overfitting
    min_samples_split=10,  # Minimum samples ƒë·ªÉ split
    random_state=42
)
dt_model.fit(X_train_processed, y_train)
models['Decision Tree'] = dt_model
print(f"  ‚úì Training complete")

# 5.3 Random Forest
print(f"\n[5.3] Training Random Forest Classifier...")
rf_model = RandomForestClassifier(
    n_estimators=100,      # S·ªë trees trong forest
    max_depth=10,          # ƒê·ªô s√¢u t·ªëi ƒëa
    min_samples_split=10,  # Minimum samples ƒë·ªÉ split
    random_state=42,
    n_jobs=-1              # S·ª≠ d·ª•ng to√†n b·ªô CPU cores
)
rf_model.fit(X_train_processed, y_train)
models['Random Forest'] = rf_model
print(f"  ‚úì Training complete")

# ============================================================================
# B∆Ø·ªöC 6: ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 6: ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET")
print("="*80)

# Dictionary ƒë·ªÉ l∆∞u results
results = {}

for model_name, model in models.items():
    print(f"\n[6.{list(models.keys()).index(model_name) + 1}] {model_name}")
    print("-" * 80)
    
    # Predictions
    y_pred = model.predict(X_test_processed)
    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]
    
    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    # L∆∞u results
    results[model_name] = {
        'model': model,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1': f1,
        'auc': auc
    }
    
    # In ra metrics
    print(f"\n  üìä METRICS:")
    print(f"    Accuracy:  {acc:.4f} ({acc*100:.2f}%)")
    print(f"    Precision: {prec:.4f} (Trong s·ªë d·ª± ƒëo√°n churn, bao nhi√™u ƒë√∫ng)")
    print(f"    Recall:    {rec:.4f} (Trong s·ªë kh√°ch h√†ng churn, d·ª± ƒëo√°n ƒë√∫ng bao nhi√™u)")
    print(f"    F1 Score:  {f1:.4f} (ƒêi·ªÉm c√¢n b·∫±ng Precision-Recall)")
    print(f"    ROC-AUC:   {auc:.4f} (Kh·∫£ nƒÉng ph√¢n bi·ªát 2 class)")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n  üìà CONFUSION MATRIX:")
    print(f"    True Negatives:  {cm[0, 0]:>3} (D·ª± ƒëo√°n Stayed - ƒê√∫ng)")
    print(f"    False Positives: {cm[0, 1]:>3} (D·ª± ƒëo√°n Churned - Sai)")
    print(f"    False Negatives: {cm[1, 0]:>3} (D·ª± ƒëo√°n Stayed - Sai)")
    print(f"    True Positives:  {cm[1, 1]:>3} (D·ª± ƒëo√°n Churned - ƒê√∫ng)")
    
    # Classification Report
    print(f"\n  üìã CLASSIFICATION REPORT:")
    print(classification_report(y_test, y_pred, 
                              target_names=['Stayed', 'Churned']))

# ============================================================================
# B∆Ø·ªöC 7: SO S√ÅNH C√ÅC M√î H√åNH
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 7: SO S√ÅNH C√ÅC M√î H√åNH")
print("="*80)

# T·∫°o b·∫£ng so s√°nh
comparison_df = pd.DataFrame({
    'Model': results.keys(),
    'Accuracy': [results[m]['accuracy'] for m in results.keys()],
    'Precision': [results[m]['precision'] for m in results.keys()],
    'Recall': [results[m]['recall'] for m in results.keys()],
    'F1 Score': [results[m]['f1'] for m in results.keys()],
    'ROC-AUC': [results[m]['auc'] for m in results.keys()]
})

print("\nüìä B·∫£ng so s√°nh:")
print(comparison_df.to_string(index=False))

# T√¨m m√¥ h√¨nh t·ªët nh·∫•t
best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']
best_accuracy = comparison_df['Accuracy'].max()

print(f"\nüèÜ M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name}")
print(f"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")

# ============================================================================
# B∆Ø·ªöC 8: VISUALIZATION
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 8: VISUALIZATION K·∫æT QU·∫¢")
print("="*80)

# T·∫°o th∆∞ m·ª•c ƒë·ªÉ l∆∞u h√¨nh
output_dir = project_root / 'notebooks'
output_dir.mkdir(exist_ok=True)

# 8.1 Confusion Matrix Heatmap
print(f"\n[8.1] Saving Confusion Matrix visualization...")
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for idx, (model_name, result) in enumerate(results.items()):
    cm = confusion_matrix(y_test, result['y_pred'])
    
    ax = axes[idx]
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=['Stayed', 'Churned'],
                yticklabels=['Stayed', 'Churned'],
                cbar=False)
    ax.set_title(f'{model_name}\nAccuracy: {result["accuracy"]:.3f}', 
                 fontweight='bold')
    ax.set_ylabel('True Label')
    ax.set_xlabel('Predicted Label')

plt.tight_layout()
plt.savefig(output_dir / 'confusion_matrices.png', dpi=200, bbox_inches='tight')
print(f"  ‚úì Saved: confusion_matrices.png")
plt.close()

# 8.2 Metrics Comparison Bar Plot
print(f"\n[8.2] Saving Metrics Comparison...")
fig, axes = plt.subplots(2, 3, figsize=(15, 8))
axes = axes.flatten()

metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']
colors = ['#3498db', '#e74c3c', '#2ecc71']

for idx, metric in enumerate(metrics):
    ax = axes[idx]
    values = comparison_df[metric].values
    models_list = comparison_df['Model'].values
    
    bars = ax.bar(models_list, values, color=colors, edgecolor='black', linewidth=1.5)
    ax.set_title(f'{metric}', fontweight='bold', fontsize=11)
    ax.set_ylabel('Score')
    ax.set_ylim([0, 1])
    ax.grid(axis='y', alpha=0.3)
    
    # Th√™m gi√° tr·ªã tr√™n c·ªôt
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # Rotate x labels
    ax.tick_params(axis='x', rotation=45)

# ·∫®n subplot cu·ªëi (kh√¥ng d√πng)
axes[4].axis('off')

plt.tight_layout()
plt.savefig(output_dir / 'metrics_comparison.png', dpi=200, bbox_inches='tight')
print(f"  ‚úì Saved: metrics_comparison.png")
plt.close()

# 8.3 ROC Curves
print(f"\n[8.3] Saving ROC Curves...")
fig, ax = plt.subplots(figsize=(10, 7))

colors_roc = ['#3498db', '#e74c3c', '#2ecc71']
for idx, (model_name, result) in enumerate(results.items()):
    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])
    auc_score = result['auc']
    ax.plot(fpr, tpr, linewidth=2.5, label=f'{model_name} (AUC={auc_score:.3f})',
            color=colors_roc[idx])

# Plot random classifier
ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC=0.5)')

ax.set_xlabel('False Positive Rate', fontsize=11)
ax.set_ylabel('True Positive Rate', fontsize=11)
ax.set_title('ROC Curves - Model Comparison', fontweight='bold', fontsize=12)
ax.legend(loc='lower right', fontsize=10)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig(output_dir / 'roc_curves.png', dpi=200, bbox_inches='tight')
print(f"  ‚úì Saved: roc_curves.png")
plt.close()

# 8.4 Feature Importance (cho Tree-based models)
print(f"\n[8.4] Saving Feature Importance...")
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Decision Tree feature importance
dt_importance = dt_model.feature_importances_
dt_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': dt_importance
}).sort_values('Importance', ascending=True)

axes[0].barh(dt_importance_df['Feature'], dt_importance_df['Importance'], 
             color='#e74c3c', edgecolor='black')
axes[0].set_title('Decision Tree - Feature Importance', fontweight='bold', fontsize=11)
axes[0].set_xlabel('Importance')
axes[0].grid(axis='x', alpha=0.3)

# Random Forest feature importance
rf_importance = rf_model.feature_importances_
rf_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_importance
}).sort_values('Importance', ascending=True)

axes[1].barh(rf_importance_df['Feature'], rf_importance_df['Importance'],
             color='#2ecc71', edgecolor='black')
axes[1].set_title('Random Forest - Feature Importance', fontweight='bold', fontsize=11)
axes[1].set_xlabel('Importance')
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig(output_dir / 'feature_importance.png', dpi=200, bbox_inches='tight')
print(f"  ‚úì Saved: feature_importance.png")
plt.close()

# 8.5 Learning Curve (cho Random Forest)
print(f"\n[8.5] Saving Learning Curve...")
train_sizes, train_scores, val_scores = learning_curve(
    rf_model,
    X_train_processed, y_train,
    cv=5,
    scoring='accuracy',
    train_sizes=np.linspace(0.1, 1.0, 10),
    n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

fig, ax = plt.subplots(figsize=(10, 6))

ax.plot(train_sizes, train_mean, label='Training Accuracy', color='#3498db', linewidth=2.5)
ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                alpha=0.2, color='#3498db')

ax.plot(train_sizes, val_mean, label='Validation Accuracy', color='#e74c3c', linewidth=2.5)
ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,
                alpha=0.2, color='#e74c3c')

ax.set_xlabel('Training Set Size', fontsize=11)
ax.set_ylabel('Accuracy', fontsize=11)
ax.set_title('Random Forest - Learning Curve', fontweight='bold', fontsize=12)
ax.legend(loc='best', fontsize=10)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig(output_dir / 'learning_curve.png', dpi=200, bbox_inches='tight')
print(f"  ‚úì Saved: learning_curve.png")
plt.close()

# ============================================================================
# B∆Ø·ªöC 9: T√ìM T·∫ÆT K·∫æT QU·∫¢
# ============================================================================

print("\n" + "="*80)
print("B∆Ø·ªöC 9: T√ìM T·∫ÆT K·∫æT QU·∫¢ & KHUY·∫æN NGH·ªä")
print("="*80)

print(f"""
üìã SUMMARY:
  ‚úì Dataset size: {len(df)} samples
  ‚úì Features: {X.shape[1]} (7 numerical + 3 categorical)
  ‚úì Train/Test split: {len(X_train)}/{len(X_test)}
  ‚úì Churn rate: {(y.sum()/len(y))*100:.1f}%
  
ü§ñ MODELS TRAINED:
  1. Logistic Regression
  2. Decision Tree Classifier
  3. Random Forest Classifier

üèÜ BEST MODEL: {best_model_name}
  - Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)
  - Precision: {results[best_model_name]['precision']:.4f}
  - Recall: {results[best_model_name]['recall']:.4f}
  - F1 Score: {results[best_model_name]['f1']:.4f}

üí° RECOMMENDATIONS:
  1. Logistic Regression l√† baseline ƒë∆°n gi·∫£n, ph√π h·ª£p h·ªçc ban ƒë·∫ßu
  2. Decision Tree d·ªÖ visualize v√† gi·∫£i th√≠ch
  3. Random Forest cho k·∫øt qu·∫£ t·ªët nh·∫•t, recommend s·ª≠ d·ª•ng
  
üìä VISUALIZATIONS SAVED:
  ‚úì confusion_matrices.png - So s√°nh confusion matrix
  ‚úì metrics_comparison.png - So s√°nh c√°c metrics
  ‚úì roc_curves.png - ROC curves cho c·∫£ 3 m√¥ h√¨nh
  ‚úì feature_importance.png - T·∫ßm quan tr·ªçng c·ªßa features
  ‚úì learning_curve.png - Learning curve c·ªßa Random Forest
  
üìÅ T·∫•t c·∫£ h√¨nh ƒë√£ l∆∞u t·∫°i: {output_dir}
""")

print("="*80)
print("‚úÖ TRAINING & EVALUATION COMPLETE!")
print("="*80 + "\n")
