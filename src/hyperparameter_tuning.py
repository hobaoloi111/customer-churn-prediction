"""
B∆Ø·ªöC 4: GRID SEARCH CV - T·ªêI ∆ØU HYPERPARAMETERS
T√¨m hyperparameters t·ªët nh·∫•t cho Random Forest
"""
# -*- coding: utf-8 -*-
import sys
import os
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')

print("\n" + "="*80)
print("B∆Ø·ªöC 4: GRID SEARCH CV - T·ªêI ∆ØU HYPERPARAMETERS")
print("="*80)

# ============================================================================
# B∆Ø·ªöC 1: LOAD & PREPARE DATA
# ============================================================================

print("\n[B∆Ø·ªöC 1] LOAD & PREPARE DATA")
print("-" * 80)

project_root = Path(__file__).parent.parent
data_file = project_root / 'data' / 'customer_churn_data.csv'

df = pd.read_csv(data_file)

# Chia X, y
target_col = 'churn'
X = df.drop(columns=['customer_id', target_col])
y = df[target_col]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Preprocessing
numerical_cols = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()

X_train_processed = X_train.copy()
X_test_processed = X_test.copy()

# Label Encoding
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X_train_processed[col] = le.fit_transform(X_train[col])
    X_test_processed[col] = le.transform(X_test[col])
    label_encoders[col] = le

# StandardScaling
scaler = StandardScaler()
X_train_processed[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test_processed[numerical_cols] = scaler.transform(X_test[numerical_cols])

print(f"‚úì Data prepared: {X_train_processed.shape}")

# ============================================================================
# B∆Ø·ªöC 2: DEFINE HYPERPARAMETER GRID
# ============================================================================

print("\n[B∆Ø·ªöC 2] DEFINE HYPERPARAMETER GRID")
print("-" * 80)

"""
Gi·∫£i th√≠ch c√°c hyperparameters Random Forest:

1. n_estimators: S·ªë l∆∞·ª£ng c√¢y quy·∫øt ƒë·ªãnh trong forest
   - C√†ng nhi·ªÅu ‚Üí D·ª± ƒëo√°n t·ªët h∆°n nh∆∞ng ch·∫≠m h∆°n
   - N√™n th·ª≠: [50, 100, 200]

2. max_depth: ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa m·ªói c√¢y
   - H·∫°n ch·∫ø ‚Üí Gi·∫£m overfitting
   - N√™n th·ª≠: [5, 10, 15, 20]

3. min_samples_split: Minimum samples y√™u c·∫ßu ƒë·ªÉ split m·ªôt node
   - Cao h∆°n ‚Üí C√¢y ƒë∆°n gi·∫£n h∆°n, gi·∫£m overfitting
   - N√™n th·ª≠: [5, 10, 20]

4. min_samples_leaf: Minimum samples c·∫ßn c√≥ ·ªü leaf node
   - Cao h∆°n ‚Üí C√¢y ƒë∆°n gi·∫£n h∆°n
   - N√™n th·ª≠: [2, 4, 8]

5. max_features: S·ªë features xem x√©t khi split
   - Gi·∫£m ‚Üí TƒÉng diversity, gi·∫£m correlation
   - N√™n th·ª≠: ['sqrt', 'log2', None]
"""

param_grid = {
    'n_estimators': [50, 100, 150],      # S·ªë c√¢y
    'max_depth': [7, 10, 15],            # ƒê·ªô s√¢u
    'min_samples_split': [5, 10, 20],    # Min samples ƒë·ªÉ split
    'min_samples_leaf': [2, 4],          # Min samples ·ªü leaf
    'max_features': ['sqrt', 'log2']     # Features ƒë·ªÉ xem x√©t
}

print("\nHyperparameter Grid:")
for param, values in param_grid.items():
    print(f"  {param}: {values}")

total_combinations = 1
for values in param_grid.values():
    total_combinations *= len(values)

print(f"\nTotal combinations: {total_combinations}")
print(f"(V·ªõi 5-fold CV ‚Üí {total_combinations * 5} model training)")

# ============================================================================
# B∆Ø·ªöC 3: GRID SEARCH CV
# ============================================================================

print("\n[B∆Ø·ªöC 3] GRID SEARCH CV")
print("-" * 80)

# Base model
base_model = RandomForestClassifier(
    random_state=42,
    n_jobs=-1  # D√πng to√†n b·ªô CPU
)

# GridSearchCV
print("\n‚è≥ T√¨m best hyperparameters...")
print("   (ƒê√¢y l√† qu√° tr√¨nh t√≠nh to√°n n·∫∑ng, c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)")

grid_search = GridSearchCV(
    estimator=base_model,
    param_grid=param_grid,
    cv=5,                    # 5-fold cross-validation
    scoring='f1',            # D√πng F1-score l√†m metric ch√≠nh
    n_jobs=-1,              # Parallel computing
    verbose=1               # Hi·ªÉn th·ªã progress
)

grid_search.fit(X_train_processed, y_train)

print(f"\n‚úÖ Grid Search Complete!")

# ============================================================================
# B∆Ø·ªöC 4: BEST PARAMETERS & RESULTS
# ============================================================================

print("\n[B∆Ø·ªöC 4] BEST PARAMETERS & RESULTS")
print("-" * 80)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"\nüèÜ BEST PARAMETERS:")
for param, value in best_params.items():
    print(f"  {param}: {value}")

print(f"\nüéØ BEST CV SCORE (F1): {best_score:.4f}")

# ============================================================================
# B∆Ø·ªöC 5: COMPARE BEFORE vs AFTER
# ============================================================================

print("\n[B∆Ø·ªöC 5] SO S√ÅNH TR∆Ø·ªöC - SAU TUNING")
print("-" * 80)

# Original model
original_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=10,
    random_state=42,
    n_jobs=-1
)
original_model.fit(X_train_processed, y_train)

# Predictions
y_pred_original = original_model.predict(X_test_processed)
y_pred_best = best_model.predict(X_test_processed)

y_pred_proba_original = original_model.predict_proba(X_test_processed)[:, 1]
y_pred_proba_best = best_model.predict_proba(X_test_processed)[:, 1]

# Metrics
metrics_original = {
    'Accuracy': accuracy_score(y_test, y_pred_original),
    'F1-Score': f1_score(y_test, y_pred_original),
    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_original)
}

metrics_best = {
    'Accuracy': accuracy_score(y_test, y_pred_best),
    'F1-Score': f1_score(y_test, y_pred_best),
    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_best)
}

# Create comparison table
comparison_data = {
    'Metric': ['Accuracy', 'F1-Score', 'ROC-AUC'],
    'Original': [metrics_original['Accuracy'], metrics_original['F1-Score'], metrics_original['ROC-AUC']],
    'Tuned': [metrics_best['Accuracy'], metrics_best['F1-Score'], metrics_best['ROC-AUC']],
    'Improvement': [
        metrics_best['Accuracy'] - metrics_original['Accuracy'],
        metrics_best['F1-Score'] - metrics_original['F1-Score'],
        metrics_best['ROC-AUC'] - metrics_original['ROC-AUC']
    ]
}

df_comparison = pd.DataFrame(comparison_data)

print("\nüìä PERFORMANCE COMPARISON:")
print(df_comparison.to_string(index=False))

print("\nüíπ IMPROVEMENT:")
for _, row in df_comparison.iterrows():
    metric = row['Metric']
    improvement = row['Improvement']
    direction = "‚úÖ" if improvement >= 0 else "‚ùå"
    print(f"  {direction} {metric}: {improvement:+.4f} ({improvement*100:+.2f}%)")

# ============================================================================
# B∆Ø·ªöC 6: DETAILED RESULTS
# ============================================================================

print("\n[B∆Ø·ªöC 6] DETAILED CLASSIFICATION REPORT (TUNED MODEL)")
print("-" * 80)

print(classification_report(
    y_test, y_pred_best,
    target_names=['Stayed', 'Churned'],
    digits=4
))

# ============================================================================
# B∆Ø·ªöC 7: CROSS-VALIDATION SCORES
# ============================================================================

print("\n[B∆Ø·ªöC 7] CROSS-VALIDATION ANALYSIS")
print("-" * 80)

# CV scores cho best model
cv_scores = cross_val_score(
    best_model, X_train_processed, y_train,
    cv=5, scoring='f1', n_jobs=-1
)

print(f"\n5-Fold CV Scores: {[f'{score:.4f}' for score in cv_scores]}")
print(f"Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# Interpretation
std = cv_scores.std()
if std < 0.05:
    print(f"‚úÖ Model r·∫•t ·ªïn ƒë·ªãnh (std < 0.05)")
elif std < 0.10:
    print(f"üü° Model kh√° ·ªïn ƒë·ªãnh (std < 0.10)")
else:
    print(f"‚ö†Ô∏è  Model c√≥ bi·∫øn ƒë·ªông (std >= 0.10)")

# ============================================================================
# B∆Ø·ªöC 8: TOP 10 HYPERPARAMETER COMBINATIONS
# ============================================================================

print("\n[B∆Ø·ªöC 8] TOP 10 HYPERPARAMETER COMBINATIONS")
print("-" * 80)

# Get results from GridSearch
results_df = pd.DataFrame(grid_search.cv_results_)
results_df = results_df[['param_n_estimators', 'param_max_depth', 'param_min_samples_split',
                          'param_min_samples_leaf', 'param_max_features', 'mean_test_score']].copy()

results_df.columns = ['n_estimators', 'max_depth', 'min_samples_split',
                      'min_samples_leaf', 'max_features', 'CV F1-Score']

results_df = results_df.sort_values('CV F1-Score', ascending=False).head(10).reset_index(drop=True)

print("\n" + results_df.to_string())

# ============================================================================
# B∆Ø·ªöC 9: RECOMMENDATIONS
# ============================================================================

print("\n[B∆Ø·ªöC 9] RECOMMENDATIONS & BEST PRACTICES")
print("-" * 80)

recommendations = """
‚úÖ BEST PRACTICES KHI D√ôNG GRID SEARCH:

1. Scope:
   - Kh√¥ng n√™n th·ª≠ qu√° nhi·ªÅu hyperparameters (t√≠nh to√°n l√¢u)
   - Focus v√†o hyperparameters M·ªíM QUAN TR·ªåNG nh·∫•t
   - C√≥ th·ªÉ th·ª≠ RandomizedSearchCV cho kh√¥ng gian l·ªõn

2. Cross-Validation:
   - Lu√¥n d√πng CV (kh√¥ng ch·ªâ train/test split)
   - 5-fold ho·∫∑c 10-fold l√† chu·∫©n
   - Stratified CV n·∫øu d·ªØ li·ªáu imbalanced

3. Scoring:
   - Ch·ªçn scoring ph√π h·ª£p v·ªõi b√†i to√°n
   - F1 t·ªët cho imbalanced classification
   - AUC n·∫øu quan t√¢m t·ªõi ranking

4. Computational Cost:
   - GridSearchCV t√≠nh to√°n n·∫∑ng
   - D√πng n_jobs=-1 ƒë·ªÉ parallel computing
   - Xem x√©t RandomizedSearchCV n·∫øu qu√° l√¢u

5. Overfitting Check:
   - So s√°nh train vs CV scores
   - N·∫øu train >> CV ‚Üí C√≥ overfitting
   - Adjust hyperparameters ƒë·ªÉ gi·∫£m complexity

6. KH√îNG n√™n:
   - Tune hyperparameters tr√™n test set
   - Qu√° t·ªëi ∆∞u h√≥a cho train set
   - B·ªè qua cross-validation
   - D√πng qu√° nhi·ªÅu hyperparameters (curse of dimensionality)

7. Ti·∫øp theo:
   - X√°c nh·∫≠n k·∫øt qu·∫£ tr√™n test set
   - L∆∞u best model & hyperparameters
   - Deploy & monitor performance trong production
"""

print(recommendations)

# ============================================================================
# B∆Ø·ªöC 10: SAVE BEST MODEL
# ============================================================================

print("\n[B∆Ø·ªöC 10] SAVE BEST MODEL")
print("-" * 80)

import joblib

models_dir = project_root / 'models'
models_dir.mkdir(exist_ok=True)

# Save best model
model_path = models_dir / 'random_forest_tuned.pkl'
joblib.dump(best_model, model_path)

print(f"\n‚úì Best model saved: {model_path}")

# Save best hyperparameters
params_path = models_dir / 'best_hyperparameters.json'
import json

with open(params_path, 'w') as f:
    json.dump(best_params, f, indent=2)

print(f"‚úì Best hyperparameters saved: {params_path}")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*80)
print("‚úÖ B∆Ø·ªöC 4 HO√ÄN T·∫§T - GRID SEARCH CV")
print("="*80)

summary = f"""
üéØ SUMMARY:

üìä Original Model:
  - Accuracy:  {metrics_original['Accuracy']:.4f}
  - F1-Score:  {metrics_original['F1-Score']:.4f}
  - ROC-AUC:   {metrics_original['ROC-AUC']:.4f}

üèÜ Tuned Model (Best):
  - Accuracy:  {metrics_best['Accuracy']:.4f}
  - F1-Score:  {metrics_best['F1-Score']:.4f}
  - ROC-AUC:   {metrics_best['ROC-AUC']:.4f}

üíπ Improvement:
  - Accuracy:  {(metrics_best['Accuracy'] - metrics_original['Accuracy'])*100:+.2f}%
  - F1-Score:  {(metrics_best['F1-Score'] - metrics_original['F1-Score'])*100:+.2f}%
  - ROC-AUC:   {(metrics_best['ROC-AUC'] - metrics_original['ROC-AUC'])*100:+.2f}%

üéØ Best Hyperparameters:
  - n_estimators: {best_params['n_estimators']}
  - max_depth: {best_params['max_depth']}
  - min_samples_split: {best_params['min_samples_split']}
  - min_samples_leaf: {best_params['min_samples_leaf']}
  - max_features: {best_params['max_features']}

üìÅ Files Saved:
  ‚úì {model_path}
  ‚úì {params_path}

üöÄ Next Steps:
  B∆∞·ªõc 5 - Unit Tests
"""

print(summary)
